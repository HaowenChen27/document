### 中文分词

> ElasticSearch 本身自带中文分词，只是单纯把中文一个字一个字的分开，根本没有词汇的概念。实际应用中，用户都是以词汇为条件，进行查询匹配，如果能够把文章以词汇为单位切分开来，那么与用户查询的条件能更匹配，查询速度也会更快。

一般使用 [ik 分词器](https://github.com/medcl/elasticsearch-analysis-ik)

安装步骤：

下载好的zip包，解压后放到 /usr/share/elasticsearch/plugins/ 下

<font color=red>记得该目录不要放置压缩文件 </font>会导致es重启失败

重启ES

#### 分词展示

使用默认

```json
GET movie_index/_analyze
{  
  "text": "我是中国人"
}
```

结果：

```json
{
  "tokens": [
    {
      "token": "我",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<IDEOGRAPHIC>",
      "position": 0
    },
    {
      "token": "是",
      "start_offset": 1,
      "end_offset": 2,
      "type": "<IDEOGRAPHIC>",
      "position": 1
    },
    {
      "token": "中",
      "start_offset": 2,
      "end_offset": 3,
      "type": "<IDEOGRAPHIC>",
      "position": 2
    },
    {
      "token": "国",
      "start_offset": 3,
      "end_offset": 4,
      "type": "<IDEOGRAPHIC>",
      "position": 3
    },
    {
      "token": "人",
      "start_offset": 4,
      "end_offset": 5,
      "type": "<IDEOGRAPHIC>",
      "position": 4
    }
  ]
}
```

默认使用es自带的分词器，可以看出把所有的字全部分开

使用分词器

```json
GET movie_index/_analyze
{  "analyzer": "ik_smart", 
  "text": "我是中国人"
}
```

结果：

```json
{
  "tokens": [
    {
      "token": "我",
      "start_offset": 0,
      "end_offset": 1,
      "type": "CN_CHAR",
      "position": 0
    },
    {
      "token": "是",
      "start_offset": 1,
      "end_offset": 2,
      "type": "CN_CHAR",
      "position": 1
    },
    {
      "token": "中国人",
      "start_offset": 2,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 2
    }
  ]
}
```

基本达到我们的要求

但是有些词并没有分出来 比如国人、中国等

使用另一个分词器

```json
GET movie_index/_analyze
{  "analyzer": "ik_max_word", 
  "text": "我是中国人"
}
```

结果：

```json
{
  "tokens": [
    {
      "token": "我",
      "start_offset": 0,
      "end_offset": 1,
      "type": "CN_CHAR",
      "position": 0
    },
    {
      "token": "是",
      "start_offset": 1,
      "end_offset": 2,
      "type": "CN_CHAR",
      "position": 1
    },
    {
      "token": "中国人",
      "start_offset": 2,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 2
    },
    {
      "token": "中国",
      "start_offset": 2,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 3
    },
    {
      "token": "国人",
      "start_offset": 3,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 4
    }
  ]
}
```

可以看到它最大限度分出了词汇



### 自定义词库

修改/usr/share/elasticsearch/plugins/ik/config/中的IKAnalyzer.cfg.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
        <comment>IK Analyzer 扩展配置</comment>
        <!--用户可以在这里配置自己的扩展字典 -->
        <entry key="ext_dict"></entry>
         <!--用户可以在这里配置自己的扩展停止词字典-->
        <entry key="ext_stopwords"></entry>
        <!--用户可以在这里配置远程扩展字典 -->
         <entry key="remote_ext_dict">http://192.168.67.163/fenci/myword.txt</entry>
        <!--用户可以在这里配置远程扩展停止词字典-->
        <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

远程扩展字典 

分词的路径可以利用nginx发布静态资源

```json
server {
        listen  80;
        server_name  192.168.67.163;
        location /fenci/ {
           root es;
    }
   }
```

并且在/usr/local/nginx/下建/es/fenci/目录，目录下加myword.txt

然后重启es服务器，重启nginx。

更新完成后，es只会对新增的数据用新词分词。历史数据是不会重新分词的。如果想要历史数据重新分词。需要执行：

```json
POST movies_index_chn/_update_by_query?conflicts=proceed
```

